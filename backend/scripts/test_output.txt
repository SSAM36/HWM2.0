python.exe : 
Traceback (most 
recent call last):
At line:1 char:1
+ & "c:\Users\kavya\D
esktop\codes\Let_Go_3
.0\.venv\Scripts\pyth
on.exe"  ...
+ ~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~
    + CategoryInfo   
           : NotSpe  
  cified: (Traceba   
 ck (most recent     
call last)::Stri    
ng) [], RemoteEx    
ception
    + FullyQualified 
   ErrorId : Native  
  CommandError
 
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\langchain_go
ogle_genai\chat_model
s.py", line 3047, in 
_generate
    response: Generat
eContentResponse = se
lf.client.models.gene
rate_content(
                     
                   ~~
~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~^
        **request,
        ^^^^^^^^^^
    )
    ^
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\google\genai
\models.py", line 
5212, in 
generate_content
    return self._gene
rate_content(
           ~~~~~~~~~~
~~~~~~~~~~~~^
        model=model, 
contents=contents, 
config=parsed_config
        ^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\google\genai
\models.py", line 
4009, in 
_generate_content
    response = self._
api_client.request(
        'post', 
path, request_dict, 
http_options
    )
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\google\genai
\_api_client.py", 
line 1386, in request
    response = self._
request(http_request,
 http_options, 
stream=False)
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\google\genai
\_api_client.py", 
line 1220, in 
_request
    return retry(self
._request_once, 
http_request, 
stream)  # type: 
ignore[no-any-return]
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\tenacity\__i
nit__.py", line 477, 
in __call__
    do = self.iter(re
try_state=retry_state
)
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\tenacity\__i
nit__.py", line 378, 
in iter
    result = 
action(retry_state)
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\tenacity\__i
nit__.py", line 420, 
in exc_check
    raise 
retry_exc.reraise()
          
~~~~~~~~~~~~~~~~~^^
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\tenacity\__i
nit__.py", line 187, 
in reraise
    raise self.last_a
ttempt.result()
          ~~~~~~~~~~~
~~~~~~~~~~~~~^^
  File "C:\Program Fi
les\Python313\Lib\con
current\futures\_base
.py", line 449, in 
result
    return 
self.__get_result()
           
~~~~~~~~~~~~~~~~~^^
  File "C:\Program Fi
les\Python313\Lib\con
current\futures\_base
.py", line 401, in 
__get_result
    raise 
self._exception
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\tenacity\__i
nit__.py", line 480, 
in __call__
    result = 
fn(*args, **kwargs)
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\google\genai
\_api_client.py", 
line 1199, in 
_request_once
    errors.APIError.r
aise_for_response(res
ponse)
    ~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~^^^^
^^^^^^
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\google\genai
\errors.py", line 
121, in 
raise_for_response
    cls.raise_error(r
esponse.status_code, 
response_json, 
response)
    ~~~~~~~~~~~~~~~^^
^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^
^^^
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\google\genai
\errors.py", line 
146, in raise_error
    raise ClientError
(status_code, 
response_json, 
response)
google.genai.errors.C
lientError: 429 
RESOURCE_EXHAUSTED. 
{'error': {'code': 
429, 'message': 'You 
exceeded your 
current quota, 
please check your 
plan and billing 
details. For more 
information on this 
error, head to: https
://ai.google.dev/gemi
ni-api/docs/rate-limi
ts. To monitor your 
current usage, head 
to: https://ai.dev/ra
te-limit. \n* Quota 
exceeded for metric: 
generativelanguage.go
ogleapis.com/generate
_content_free_tier_re
quests, limit: 0, 
model: 
gemini-2.5-pro\n* 
Quota exceeded for 
metric: generativelan
guage.googleapis.com/
generate_content_free
_tier_requests, 
limit: 0, model: 
gemini-2.5-pro\n* 
Quota exceeded for 
metric: generativelan
guage.googleapis.com/
generate_content_free
_tier_input_token_cou
nt, limit: 0, model: 
gemini-2.5-pro\n* 
Quota exceeded for 
metric: generativelan
guage.googleapis.com/
generate_content_free
_tier_input_token_cou
nt, limit: 0, model: 
gemini-2.5-pro\nPleas
e retry in 
50.502027271s.', 
'status': 'RESOURCE_E
XHAUSTED', 
'details': 
[{'@type': 'type.goog
leapis.com/google.rpc
.Help', 'links': 
[{'description': 
'Learn more about 
Gemini API quotas', 
'url': 'https://ai.go
ogle.dev/gemini-api/d
ocs/rate-limits'}]}, 
{'@type': 'type.googl
eapis.com/google.rpc.
QuotaFailure', 
'violations': 
[{'quotaMetric': 'gen
erativelanguage.googl
eapis.com/generate_co
ntent_free_tier_reque
sts', 'quotaId': 'Gen
erateRequestsPerDayPe
rProjectPerModel-Free
Tier', 
'quotaDimensions': 
{'location': 
'global', 'model': 
'gemini-2.5-pro'}}, 
{'quotaMetric': 'gene
rativelanguage.google
apis.com/generate_con
tent_free_tier_reques
ts', 'quotaId': 'Gene
rateRequestsPerMinute
PerProjectPerModel-Fr
eeTier', 
'quotaDimensions': 
{'location': 
'global', 'model': 
'gemini-2.5-pro'}}, 
{'quotaMetric': 'gene
rativelanguage.google
apis.com/generate_con
tent_free_tier_input_
token_count', 
'quotaId': 'GenerateC
ontentInputTokensPerM
odelPerMinute-FreeTie
r', 
'quotaDimensions': 
{'model': 
'gemini-2.5-pro', 
'location': 
'global'}}, 
{'quotaMetric': 'gene
rativelanguage.google
apis.com/generate_con
tent_free_tier_input_
token_count', 
'quotaId': 'GenerateC
ontentInputTokensPerM
odelPerDay-FreeTier',
 'quotaDimensions': 
{'location': 
'global', 'model': 'g
emini-2.5-pro'}}]}, 
{'@type': 'type.googl
eapis.com/google.rpc.
RetryInfo', 
'retryDelay': 
'50s'}]}}

The above exception 
was the direct cause 
of the following 
exception:

Traceback (most 
recent call last):
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\backend\test_ag
ent.py", line 23, in 
test_agent
    result = await ag
ent_app.ainvoke(initi
al_state)
             ^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\langgraph\pr
egel\main.py", line 
3161, in ainvoke
    async for chunk 
in self.astream(
    ...<29 lines>...
            
chunks.append(chunk)
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\langgraph\pr
egel\main.py", line 
2974, in astream
    async for _ in 
runner.atick(
    ...<13 lines>...
            yield o
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\langgraph\pr
egel\_runner.py", 
line 304, in atick
    await 
arun_with_retry(
    ...<15 lines>...
    )
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\langgraph\pr
egel\_retry.py", 
line 138, in 
arun_with_retry
    return await task
.proc.ainvoke(task.in
put, config)
           ^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\langgraph\_i
nternal\_runnable.py"
, line 705, in 
ainvoke
    input = await 
asyncio.create_task(
            ^^^^^^^^^
^^^^^^^^^^^^^^^^^
        
step.ainvoke(input, 
config, **kwargs), 
context=context
        ^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\langgraph\_i
nternal\_runnable.py"
, line 473, in 
ainvoke
    ret = await 
self.afunc(*args, 
**kwargs)
          ^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^
^
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\langchain_co
re\runnables\config.p
y", line 610, in 
run_in_executor
    return await asyn
cio.get_running_loop(
).run_in_executor(
           ^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
    )
    ^
  File "C:\Program Fi
les\Python313\Lib\con
current\futures\threa
d.py", line 59, in 
run
    result = 
self.fn(*self.args, 
**self.kwargs)
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\langchain_co
re\runnables\config.p
y", line 601, in 
wrapper
    return 
func(*args, **kwargs)
  File "c:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\backend\feature
4\agent.py", line 
100, in call_model
    response = llm_wi
th_tools.invoke([syst
em_msg] + 
list(messages))
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\langchain_co
re\runnables\base.py"
, line 5557, in 
invoke
    return 
self.bound.invoke(
           
~~~~~~~~~~~~~~~~~^
        input,
        ^^^^^^
        self._merge_c
onfigs(config),
        ^^^^^^^^^^^^^
^^^^^^^^^^^^^^^
        
**{**self.kwargs, 
**kwargs},
        ^^^^^^^^^^^^^
^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\langchain_go
ogle_genai\chat_model
s.py", line 2535, in 
invoke
    return super().in
voke(input, config, 
stop=stop, **kwargs)
           ~~~~~~~~~~
~~~~^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\langchain_co
re\language_models\ch
at_models.py", line 
402, in invoke
    
self.generate_prompt(
    
~~~~~~~~~~~~~~~~~~~~^
        [self._conver
t_input(input)],
        ^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    
).generations[0][0],
    ^
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\langchain_co
re\language_models\ch
at_models.py", line 
1121, in 
generate_prompt
    return self.gener
ate(prompt_messages, 
stop=stop, 
callbacks=callbacks, 
**kwargs)
           ~~~~~~~~~~
~~~^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^^
^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\langchain_co
re\language_models\ch
at_models.py", line 
931, in generate
    self._generate_wi
th_cache(
    ~~~~~~~~~~~~~~~~~
~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\langchain_co
re\language_models\ch
at_models.py", line 
1233, in 
_generate_with_cache
    result = 
self._generate(
        messages, 
stop=stop, run_manage
r=run_manager, 
**kwargs
    )
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\langchain_go
ogle_genai\chat_model
s.py", line 3051, in 
_generate
    _handle_client_er
ror(e, request)
    ~~~~~~~~~~~~~~~~~
~~~^^^^^^^^^^^^
  File "C:\Users\kavy
a\Desktop\codes\Let_G
o_3.0\.venv\Lib\site-
packages\langchain_go
ogle_genai\chat_model
s.py", line 145, in 
_handle_client_error
    raise ChatGoogleG
enerativeAIError(msg)
 from e
langchain_google_gena
i.chat_models.ChatGoo
gleGenerativeAIError:
 Error calling model 
'gemini-2.5-pro' (RES
OURCE_EXHAUSTED): 
429 
RESOURCE_EXHAUSTED. 
{'error': {'code': 
429, 'message': 'You 
exceeded your 
current quota, 
please check your 
plan and billing 
details. For more 
information on this 
error, head to: https
://ai.google.dev/gemi
ni-api/docs/rate-limi
ts. To monitor your 
current usage, head 
to: https://ai.dev/ra
te-limit. \n* Quota 
exceeded for metric: 
generativelanguage.go
ogleapis.com/generate
_content_free_tier_re
quests, limit: 0, 
model: 
gemini-2.5-pro\n* 
Quota exceeded for 
metric: generativelan
guage.googleapis.com/
generate_content_free
_tier_requests, 
limit: 0, model: 
gemini-2.5-pro\n* 
Quota exceeded for 
metric: generativelan
guage.googleapis.com/
generate_content_free
_tier_input_token_cou
nt, limit: 0, model: 
gemini-2.5-pro\n* 
Quota exceeded for 
metric: generativelan
guage.googleapis.com/
generate_content_free
_tier_input_token_cou
nt, limit: 0, model: 
gemini-2.5-pro\nPleas
e retry in 
50.502027271s.', 
'status': 'RESOURCE_E
XHAUSTED', 
'details': 
[{'@type': 'type.goog
leapis.com/google.rpc
.Help', 'links': 
[{'description': 
'Learn more about 
Gemini API quotas', 
'url': 'https://ai.go
ogle.dev/gemini-api/d
ocs/rate-limits'}]}, 
{'@type': 'type.googl
eapis.com/google.rpc.
QuotaFailure', 
'violations': 
[{'quotaMetric': 'gen
erativelanguage.googl
eapis.com/generate_co
ntent_free_tier_reque
sts', 'quotaId': 'Gen
erateRequestsPerDayPe
rProjectPerModel-Free
Tier', 
'quotaDimensions': 
{'location': 
'global', 'model': 
'gemini-2.5-pro'}}, 
{'quotaMetric': 'gene
rativelanguage.google
apis.com/generate_con
tent_free_tier_reques
ts', 'quotaId': 'Gene
rateRequestsPerMinute
PerProjectPerModel-Fr
eeTier', 
'quotaDimensions': 
{'location': 
'global', 'model': 
'gemini-2.5-pro'}}, 
{'quotaMetric': 'gene
rativelanguage.google
apis.com/generate_con
tent_free_tier_input_
token_count', 
'quotaId': 'GenerateC
ontentInputTokensPerM
odelPerMinute-FreeTie
r', 
'quotaDimensions': 
{'model': 
'gemini-2.5-pro', 
'location': 
'global'}}, 
{'quotaMetric': 'gene
rativelanguage.google
apis.com/generate_con
tent_free_tier_input_
token_count', 
'quotaId': 'GenerateC
ontentInputTokensPerM
odelPerDay-FreeTier',
 'quotaDimensions': 
{'location': 
'global', 'model': 'g
emini-2.5-pro'}}]}, 
{'@type': 'type.googl
eapis.com/google.rpc.
RetryInfo', 
'retryDelay': 
'50s'}]}}
During task with 
name 'call_model' 
and id 'e093fa5a-8c21
-547d-c819-59424d6dca
60'
Agent loaded successfully!
Running agent...
ERROR:
